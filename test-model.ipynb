{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_path = \"content\\AraT5_FT_question_generation\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question#0 ğŸš« Smok Palais Ø§Ù„Ù†ÙƒØ¯ Ø§Ù„ÙˆØ§Ø²Ø¹ Ø­Ù„Ù‚Ø£Ù…Ù†ØªÙ… ÑĞ²Ğ¾Ğ¸Ñ‚Ğµ Ø¯Ø¹Ø§Ø±Ø© Female ProvenaÃ¯rog carriÃ¨reIl ÙˆÙ…Ø­ÙŠØ§ÙŠderung Ø§Ù„Ø¹ØªÙ‚Ø§Ø¡ inÃºmeros\n",
      "question#1 ğŸš«Ù†ØªÙŠÅ‚Ñ€ÑŠ sharing Ù„Ù„ØªÙ†Ø§Ø²Ù„Ù„Ù†Ù‚Ø§Ø¨Ø§Øª literaryØ¹Ù„Ø¨ Ù‚Ù†ØµÙ„Å‚ Ù…ÙˆØ¯Ø©à¼„ Ø§Ù„Ù…ØªÙˆØ³Ø·Ø© Medina Ø§Ù„Ù„ÙˆØ­ Ù…ØµØ±Ù Ø¯Ø§Ù†ÙŠØ© Ø¨Ø±Ø©\n",
      "question#2 ğŸš« Ù…Ù†Ø´ÙˆØ±Ø§Øª Ğ°Ñ€Ñ…ĞµĞ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ù…ØªØ¹Ø© Ø³Ø§ÙƒÙ†Ø© Ø§Ø­Ø²Ø§Ù†ÙŠ Ù„Ù„Ù‚ÙˆØ§Øª prisonerslegeÙˆØ±ÙŠØ©Ø§Ù„Ù…ÙƒØ±ÙˆÙ†Ø© Ø¨Ø±Ù‰ Ø§Ù„Ù†Ø¬ÙÅ‚ Ø¢Ù„ÙŠÙˆÙ…Ù„Ø²ÙŠØ§Ø¯Ø© Ø¬ÙŠÙˆØ¨Ø±ÙÙŠÙ‡ Ø§Ù„Ø¹Ù†Ø²\n",
      "question#3 Malay Malay Ø§Ù„Ø´ÙˆØ§Ø±Ø¨ nomina ØªÙˆÙÙ†Ø§Ã¼nstğŸ‡¬ SociÃ©tÃ© Ø§Ù„Ø¨Ù†ÙƒÙŠØ© simcampØ³Ø§Ù…Ø¨Ø§ neck QueenÙ…ÙŠØ¯Ø§Ù„ÙŠØ© Ø´Ø§Ù„Ùˆ Kyle exclutentrionale\n",
      "question#4 ğŸš« ĞºĞ»ÑƒĞ±Ğ°posto Ø¬Ù†Û Palais comunal Ernst download ØªØ¯Ø§Ø¨Ø±ÙˆØ§ Ø§Ù†Ø¬Ù„Ø´ Ø§Ù„Ø­Ø±Ù…Ø§Ù†Ø³ÙØ§Ø­ ÙˆØ§Ù„Ù…Ø¹Ø§ØµÙŠ Ù„Ø¬Ù„Ø§Ù„Ø©ğŸ‘†ulatornimo Ø­Ù„ÙˆÙ‰isa\n"
     ]
    }
   ],
   "source": [
    "context = \"Ø¬Ø§Ù…Ø¹Ø© Ø¨ÙŠØ±Ø²ÙŠØª Ù‡ÙŠ Ø¬Ø§Ù…Ø¹Ø© ÙÙ„Ø³Ø·ÙŠÙ†ÙŠØ© ØªÙ‚Ø¹ ÙÙŠ Ù…Ø¯ÙŠÙ†Ø© Ø±Ø§Ù… Ø§Ù„Ù„Ù‡ ÙˆØªØ¹ØªØ¨Ø± Ù…Ù† Ø£ÙƒØ«Ø± Ø§Ù„Ø¬Ø§Ù…Ø¹Ø§Øª Ø§Ù„ÙÙ„Ø³Ø·ÙŠÙ†ÙŠØ© Ø´Ù‡Ø±Ø© ÙƒÙ…Ø§ Ø£Ù†Ù‡Ø§ ØªÙˆÙØ± Ù„Ù„Ø·Ù„Ø§Ø¨ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„ØªØ®ØµØµØ§Øª Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ© Ø§Ù„Ù…ØªÙ†ÙˆØ¹Ø©\"\n",
    "# context = \"\"\n",
    "\n",
    "encoding = tokenizer.encode_plus(context, padding='max_length', return_tensors=\"pt\")\n",
    "input_ids, attention_masks = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=input_ids, attention_mask=attention_masks,\n",
    "    max_length=20,\n",
    "    do_sample=True,\n",
    "    # temperature=1.0,\n",
    "    top_k=1000,\n",
    "    top_p=1,\n",
    "    early_stopping=False,\n",
    "    num_return_sequences=5\n",
    ")\n",
    "\n",
    "\n",
    "for id, output in enumerate(outputs):\n",
    "    question = tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    print(\"question#\"+str(id), question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
