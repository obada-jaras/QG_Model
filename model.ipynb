{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.5.1 nltk datasets regex scikit-learn sentencepiece protobuf fairscale sacrebleu rouge_score -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 36637  100 36637    0     0  69936      0 --:--:-- --:--:-- --:--:-- 70591\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  7689  100  7689    0     0   8658      0 --:--:-- --:--:-- --:--:--  8717\n",
      "100  7689  100  7689    0     0   8657      0 --:--:-- --:--:-- --:--:--  8717\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/UBC-NLP/araT5/main/examples/run_trainier_seq2seq_huggingface.py?token=AA4R7KKKCBPS5WX4BJSWNFTBG2OPK -o run_trainier_seq2seq_huggingface.py\n",
    "!curl https://raw.githubusercontent.com/UBC-NLP/araT5/main/examples/eval_squad.py?token=AA4R7KOBBXSPCDXRRHJ3RW3BG2RF4 -o eval_squad.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_checkpoint None\n",
      "04/06/2023 04:35:32 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
      "04/06/2023 04:35:32 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=True,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=D:/New folder/Model/content/AraT5_FT_question_generation\\runs\\Apr06_04-35-32_LAPTOP-NQ25E69Q,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=epoch,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=eval_bleu,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=D:/New folder/Model/content/AraT5_FT_question_generation,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=D:/New folder/Model/content/AraT5_FT_question_generation,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "04/06/2023 04:35:33 - WARNING - datasets.builder -   Found cached dataset json (d:/tmp/AraT5_cache_dir/json/default-b59a04ea7a6236d9/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n",
      "04/06/2023 04:40:27 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at d:\\tmp\\AraT5_cache_dir\\json\\default-b59a04ea7a6236d9\\0.0.0\\fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\\cache-d6027ecf3f030aeb.arrow\n",
      "04/06/2023 04:40:27 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at d:\\tmp\\AraT5_cache_dir\\json\\default-b59a04ea7a6236d9\\0.0.0\\fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\\cache-17d5ad72b33f0896.arrow\n",
      "[INFO] evlaute using  bleu score task name: question_generation\n",
      "[INFO] early_stopping_num= 20\n",
      "***** checkpoint= None\n",
      "{'loss': 34.8319, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n",
      "{'eval_loss': 26.341596603393555, 'eval_bleu': 0.0, 'eval_gen_len': 19.0, 'eval_runtime': 118.5277, 'eval_samples_per_second': 0.844, 'eval_steps_per_second': 0.11, 'epoch': 1.0}\n",
      "{'loss': 18.1129, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n",
      "{'eval_loss': 14.074260711669922, 'eval_bleu': 0.0, 'eval_gen_len': 15.69, 'eval_runtime': 104.1757, 'eval_samples_per_second': 0.96, 'eval_steps_per_second': 0.125, 'epoch': 2.0}\n",
      "{'loss': 13.358, 'learning_rate': 0.0, 'epoch': 3.0}\n",
      "{'eval_loss': 12.144558906555176, 'eval_bleu': 0.0, 'eval_gen_len': 5.06, 'eval_runtime': 95.3768, 'eval_samples_per_second': 1.048, 'eval_steps_per_second': 0.136, 'epoch': 3.0}\n",
      "{'train_runtime': 5679.7065, 'train_samples_per_second': 0.264, 'train_steps_per_second': 0.033, 'train_loss': 22.100942137380127, 'epoch': 3.0}\n",
      "04/06/2023 06:17:40 - INFO - __main__ -   ***** train metrics *****\n",
      "04/06/2023 06:17:42 - INFO - __main__ -     epoch                    =        3.0\n",
      "04/06/2023 06:17:42 - INFO - __main__ -     train_loss               =    22.1009\n",
      "04/06/2023 06:17:42 - INFO - __main__ -     train_runtime            = 1:34:39.70\n",
      "04/06/2023 06:17:42 - INFO - __main__ -     train_samples            =        500\n",
      "04/06/2023 06:17:42 - INFO - __main__ -     train_samples_per_second =      0.264\n",
      "04/06/2023 06:17:42 - INFO - __main__ -     train_steps_per_second   =      0.033\n",
      "04/06/2023 06:17:46 - INFO - __main__ -   *** Evaluate ***\n",
      "04/06/2023 06:24:56 - INFO - __main__ -   ***** val metrics *****\n",
      "04/06/2023 06:24:57 - INFO - __main__ -     epoch                   =        3.0\n",
      "04/06/2023 06:24:57 - INFO - __main__ -     eval_bleu               =        0.0\n",
      "04/06/2023 06:24:57 - INFO - __main__ -     eval_check_point        = AraT5-base\n",
      "04/06/2023 06:24:57 - INFO - __main__ -     eval_gen_len            =      127.0\n",
      "04/06/2023 06:24:57 - INFO - __main__ -     eval_loss               =    26.3416\n",
      "04/06/2023 06:24:57 - INFO - __main__ -     eval_runtime            = 0:06:56.33\n",
      "04/06/2023 06:24:57 - INFO - __main__ -     eval_samples            =        100\n",
      "04/06/2023 06:24:57 - INFO - __main__ -     eval_samples_per_second =       0.24\n",
      "04/06/2023 06:24:57 - INFO - __main__ -     eval_steps_per_second   =      0.031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 334.10it/s]\n",
      "loading configuration file config.json from cache at /tmp/AraT5_cache_dir\\models--UBC-NLP--AraT5-base\\snapshots\\ed49be981b4df4040e83de16fd559e191b87429f\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"UBC-NLP/AraT5-base\",\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"transformers_version\": \"4.27.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 110080\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /tmp/AraT5_cache_dir\\models--UBC-NLP--AraT5-base\\snapshots\\ed49be981b4df4040e83de16fd559e191b87429f\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"UBC-NLP/AraT5-base\",\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"transformers_version\": \"4.27.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 110080\n",
      "}\n",
      "\n",
      "loading file spiece.model from cache at /tmp/AraT5_cache_dir\\models--UBC-NLP--AraT5-base\\snapshots\\ed49be981b4df4040e83de16fd559e191b87429f\\spiece.model\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /tmp/AraT5_cache_dir\\models--UBC-NLP--AraT5-base\\snapshots\\ed49be981b4df4040e83de16fd559e191b87429f\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /tmp/AraT5_cache_dir\\models--UBC-NLP--AraT5-base\\snapshots\\ed49be981b4df4040e83de16fd559e191b87429f\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at /tmp/AraT5_cache_dir\\models--UBC-NLP--AraT5-base\\snapshots\\ed49be981b4df4040e83de16fd559e191b87429f\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"UBC-NLP/AraT5-base\",\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"transformers_version\": \"4.27.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 110080\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /tmp/AraT5_cache_dir\\models--UBC-NLP--AraT5-base\\snapshots\\ed49be981b4df4040e83de16fd559e191b87429f\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"UBC-NLP/AraT5-base\",\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"transformers_version\": \"4.27.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 110080\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /tmp/AraT5_cache_dir\\models--UBC-NLP--AraT5-base\\snapshots\\ed49be981b4df4040e83de16fd559e191b87429f\\pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.27.4\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at UBC-NLP/AraT5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "Generation config file not found, using a generation config created from the model config.\n",
      "c:\\Users\\obada\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 500\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 189\n",
      "  Number of trainable parameters = 282770688\n",
      "\n",
      "  0%|          | 0/189 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      "  1%|          | 1/189 [00:15<46:11, 14.74s/it]\n",
      "  1%|          | 2/189 [20:14<36:56:36, 711.21s/it]\n",
      "  2%|▏         | 3/189 [25:53<27:59:32, 541.79s/it]\n",
      "  2%|▏         | 4/189 [26:45<17:53:26, 348.14s/it]\n",
      "  3%|▎         | 5/189 [27:19<12:00:35, 234.98s/it]\n",
      "  3%|▎         | 6/189 [27:45<8:19:31, 163.78s/it] \n",
      "  4%|▎         | 7/189 [27:58<5:47:36, 114.60s/it]\n",
      "  4%|▍         | 8/189 [28:12<4:09:04, 82.57s/it] \n",
      "  5%|▍         | 9/189 [28:23<3:01:12, 60.41s/it]\n",
      "  5%|▌         | 10/189 [28:33<2:13:11, 44.65s/it]\n",
      "  6%|▌         | 11/189 [28:44<1:42:12, 34.45s/it]\n",
      "  6%|▋         | 12/189 [28:52<1:17:58, 26.43s/it]\n",
      "  7%|▋         | 13/189 [29:17<1:16:18, 26.02s/it]\n",
      "  7%|▋         | 14/189 [29:29<1:03:21, 21.72s/it]\n",
      "  8%|▊         | 15/189 [29:43<55:43, 19.22s/it]  \n",
      "  8%|▊         | 16/189 [29:51<46:19, 16.06s/it]\n",
      "  9%|▉         | 17/189 [30:25<1:00:46, 21.20s/it]\n",
      " 10%|▉         | 18/189 [30:39<54:58, 19.29s/it]  \n",
      " 10%|█         | 19/189 [30:52<48:57, 17.28s/it]\n",
      " 11%|█         | 20/189 [31:01<41:53, 14.88s/it]\n",
      " 11%|█         | 21/189 [31:08<34:56, 12.48s/it]\n",
      " 12%|█▏        | 22/189 [31:46<55:45, 20.04s/it]\n",
      " 12%|█▏        | 23/189 [31:59<49:28, 17.88s/it]\n",
      " 13%|█▎        | 24/189 [32:07<41:13, 14.99s/it]\n",
      " 13%|█▎        | 25/189 [32:16<36:16, 13.27s/it]\n",
      " 14%|█▍        | 26/189 [32:26<33:36, 12.37s/it]\n",
      " 14%|█▍        | 27/189 [32:36<31:25, 11.64s/it]\n",
      " 15%|█▍        | 28/189 [32:45<28:57, 10.79s/it]\n",
      " 15%|█▌        | 29/189 [32:58<30:32, 11.45s/it]\n",
      " 16%|█▌        | 30/189 [33:10<30:50, 11.64s/it]\n",
      " 16%|█▋        | 31/189 [33:20<29:31, 11.21s/it]\n",
      " 17%|█▋        | 32/189 [33:31<28:39, 10.95s/it]\n",
      " 17%|█▋        | 33/189 [33:40<27:17, 10.49s/it]\n",
      " 18%|█▊        | 34/189 [33:51<27:06, 10.50s/it]\n",
      " 19%|█▊        | 35/189 [34:02<27:18, 10.64s/it]\n",
      " 19%|█▉        | 36/189 [34:12<27:17, 10.70s/it]\n",
      " 20%|█▉        | 37/189 [35:10<1:02:08, 24.53s/it]\n",
      " 20%|██        | 38/189 [35:23<53:52, 21.41s/it]  \n",
      " 21%|██        | 39/189 [35:33<44:40, 17.87s/it]\n",
      " 21%|██        | 40/189 [36:15<1:02:04, 25.00s/it]\n",
      " 22%|██▏       | 41/189 [36:30<54:55, 22.27s/it]  \n",
      " 22%|██▏       | 42/189 [36:39<44:29, 18.16s/it]\n",
      " 23%|██▎       | 43/189 [37:18<59:23, 24.41s/it]\n",
      " 23%|██▎       | 44/189 [37:30<50:08, 20.75s/it]\n",
      " 24%|██▍       | 45/189 [37:40<42:05, 17.54s/it]\n",
      " 24%|██▍       | 46/189 [37:49<35:23, 14.85s/it]\n",
      " 25%|██▍       | 47/189 [38:40<1:00:30, 25.57s/it]\n",
      " 25%|██▌       | 48/189 [38:55<53:12, 22.64s/it]  \n",
      " 26%|██▌       | 49/189 [39:02<41:51, 17.94s/it]\n",
      " 26%|██▋       | 50/189 [39:16<38:33, 16.65s/it]\n",
      " 27%|██▋       | 51/189 [39:25<33:23, 14.52s/it]\n",
      " 28%|██▊       | 52/189 [39:35<29:47, 13.05s/it]\n",
      " 28%|██▊       | 53/189 [39:46<27:54, 12.31s/it]\n",
      " 29%|██▊       | 54/189 [40:20<42:37, 18.95s/it]\n",
      " 29%|██▉       | 55/189 [40:35<39:32, 17.71s/it]\n",
      " 30%|██▉       | 56/189 [40:44<33:40, 15.19s/it]\n",
      " 30%|███       | 57/189 [40:54<30:10, 13.72s/it]\n",
      " 31%|███       | 58/189 [41:05<27:58, 12.82s/it]\n",
      " 31%|███       | 59/189 [41:16<26:23, 12.18s/it]\n",
      " 32%|███▏      | 60/189 [41:32<28:50, 13.42s/it]\n",
      " 32%|███▏      | 61/189 [41:45<28:12, 13.22s/it]\n",
      " 33%|███▎      | 62/189 [42:04<31:47, 15.02s/it]\n",
      " 33%|███▎      | 63/189 [42:15<28:50, 13.74s/it]\n",
      "                                                \n",
      "\n",
      " 33%|███▎      | 63/189 [42:21<28:50, 13.74s/it]***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.27.4\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      " 15%|█▌        | 2/13 [00:07<00:42,  3.86s/it]\u001b[A\n",
      "\n",
      " 23%|██▎       | 3/13 [00:14<00:51,  5.18s/it]\u001b[A\n",
      "\n",
      " 31%|███       | 4/13 [00:20<00:50,  5.56s/it]\u001b[A\n",
      "\n",
      " 38%|███▊      | 5/13 [00:28<00:48,  6.11s/it]\u001b[A\n",
      "\n",
      " 46%|████▌     | 6/13 [00:35<00:45,  6.55s/it]\u001b[A\n",
      "\n",
      " 54%|█████▍    | 7/13 [00:41<00:38,  6.41s/it]\u001b[A\n",
      "\n",
      " 62%|██████▏   | 8/13 [00:48<00:33,  6.67s/it]\u001b[A\n",
      "\n",
      " 69%|██████▉   | 9/13 [00:54<00:24,  6.23s/it]\u001b[A\n",
      "\n",
      " 77%|███████▋  | 10/13 [01:01<00:19,  6.45s/it]\u001b[A\n",
      "\n",
      " 85%|████████▍ | 11/13 [01:09<00:13,  6.92s/it]\u001b[A\n",
      "\n",
      " 92%|█████████▏| 12/13 [01:16<00:07,  7.00s/it]\u001b[A\n",
      "\n",
      "100%|██████████| 13/13 [01:20<00:00,  6.26s/it]\u001b[A\n",
      "                                                \n",
      "\n",
      "\n",
      "                                               \n",
      "\u001b[A\n",
      " 33%|███▎      | 63/189 [44:43<28:50, 13.74s/it]\n",
      "\n",
      "100%|██████████| 13/13 [01:40<00:00,  6.26s/it]\u001b[A\n",
      "\n",
      "                                               \u001b[ASaving model checkpoint to D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-63\n",
      "Configuration saved in D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-63\\config.json\n",
      "Configuration saved in D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-63\\generation_config.json\n",
      "Model weights saved in D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-63\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-63\\tokenizer_config.json\n",
      "Special tokens file saved in D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-63\\special_tokens_map.json\n",
      "Copy vocab file to D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-63\\spiece.model\n",
      "\n",
      " 34%|███▍      | 64/189 [52:12<6:32:39, 188.48s/it]\n",
      " 34%|███▍      | 65/189 [53:50<5:33:40, 161.46s/it]\n",
      " 35%|███▍      | 66/189 [54:02<3:59:17, 116.73s/it]\n",
      " 35%|███▌      | 67/189 [54:08<2:49:36, 83.42s/it] \n",
      " 36%|███▌      | 68/189 [54:14<2:01:20, 60.17s/it]\n",
      " 37%|███▋      | 69/189 [54:21<1:28:43, 44.36s/it]\n",
      " 37%|███▋      | 70/189 [54:43<1:14:58, 37.80s/it]\n",
      " 38%|███▊      | 71/189 [54:56<59:11, 30.10s/it]  \n",
      " 38%|███▊      | 72/189 [55:03<45:16, 23.22s/it]\n",
      " 39%|███▊      | 73/189 [55:13<37:25, 19.35s/it]\n",
      " 39%|███▉      | 74/189 [55:20<29:41, 15.49s/it]\n",
      " 40%|███▉      | 75/189 [55:43<33:46, 17.78s/it]\n",
      " 40%|████      | 76/189 [55:54<29:46, 15.81s/it]\n",
      " 41%|████      | 77/189 [56:01<24:40, 13.21s/it]\n",
      " 41%|████▏     | 78/189 [56:09<21:34, 11.67s/it]\n",
      " 42%|████▏     | 79/189 [57:37<1:03:09, 34.45s/it]\n",
      " 42%|████▏     | 80/189 [57:47<49:13, 27.10s/it]  \n",
      " 43%|████▎     | 81/189 [57:56<38:52, 21.59s/it]\n",
      " 43%|████▎     | 82/189 [58:05<31:49, 17.84s/it]\n",
      " 44%|████▍     | 83/189 [58:41<41:28, 23.47s/it]\n",
      " 44%|████▍     | 84/189 [58:54<35:38, 20.37s/it]\n",
      " 45%|████▍     | 85/189 [59:01<28:05, 16.21s/it]\n",
      " 46%|████▌     | 86/189 [59:08<23:17, 13.57s/it]\n",
      " 46%|████▌     | 87/189 [59:29<26:52, 15.81s/it]\n",
      " 47%|████▋     | 88/189 [59:39<23:41, 14.07s/it]\n",
      " 47%|████▋     | 89/189 [59:48<20:49, 12.50s/it]\n",
      " 48%|████▊     | 90/189 [59:58<19:17, 11.69s/it]\n",
      " 48%|████▊     | 91/189 [1:00:26<27:12, 16.66s/it]\n",
      " 49%|████▊     | 92/189 [1:00:42<26:37, 16.46s/it]\n",
      " 49%|████▉     | 93/189 [1:00:48<21:20, 13.34s/it]\n",
      " 50%|████▉     | 94/189 [1:00:54<17:41, 11.18s/it]\n",
      " 50%|█████     | 95/189 [1:01:10<19:49, 12.66s/it]\n",
      " 51%|█████     | 96/189 [1:01:27<21:20, 13.77s/it]\n",
      " 51%|█████▏    | 97/189 [1:01:35<18:29, 12.06s/it]\n",
      " 52%|█████▏    | 98/189 [1:02:05<26:18, 17.35s/it]\n",
      " 52%|█████▏    | 99/189 [1:02:13<22:09, 14.78s/it]\n",
      " 53%|█████▎    | 100/189 [1:02:21<18:54, 12.75s/it]\n",
      " 53%|█████▎    | 101/189 [1:02:33<18:05, 12.34s/it]\n",
      " 54%|█████▍    | 102/189 [1:03:02<24:58, 17.23s/it]\n",
      " 54%|█████▍    | 103/189 [1:03:14<22:41, 15.83s/it]\n",
      " 55%|█████▌    | 104/189 [1:03:21<18:37, 13.14s/it]\n",
      " 56%|█████▌    | 105/189 [1:03:27<15:18, 10.93s/it]\n",
      " 56%|█████▌    | 106/189 [1:03:41<16:34, 11.99s/it]\n",
      " 57%|█████▋    | 107/189 [1:03:56<17:38, 12.91s/it]\n",
      " 57%|█████▋    | 108/189 [1:04:02<14:39, 10.86s/it]\n",
      " 58%|█████▊    | 109/189 [1:04:14<14:37, 10.97s/it]\n",
      " 58%|█████▊    | 110/189 [1:04:23<13:46, 10.46s/it]\n",
      " 59%|█████▊    | 111/189 [1:04:32<13:20, 10.27s/it]\n",
      " 59%|█████▉    | 112/189 [1:04:57<18:26, 14.37s/it]\n",
      " 60%|█████▉    | 113/189 [1:05:03<15:21, 12.13s/it]\n",
      " 60%|██████    | 114/189 [1:05:12<13:53, 11.11s/it]\n",
      " 61%|██████    | 115/189 [1:05:20<12:32, 10.17s/it]\n",
      " 61%|██████▏   | 116/189 [1:05:48<18:44, 15.41s/it]\n",
      " 62%|██████▏   | 117/189 [1:06:02<18:01, 15.03s/it]\n",
      " 62%|██████▏   | 118/189 [1:06:10<15:25, 13.04s/it]\n",
      " 63%|██████▎   | 119/189 [1:06:18<13:13, 11.34s/it]\n",
      " 63%|██████▎   | 120/189 [1:06:26<12:05, 10.51s/it]\n",
      " 64%|██████▍   | 121/189 [1:06:34<11:02,  9.74s/it]\n",
      " 65%|██████▍   | 122/189 [1:06:45<11:14, 10.07s/it]\n",
      " 65%|██████▌   | 123/189 [1:06:54<10:36,  9.65s/it]\n",
      " 66%|██████▌   | 124/189 [1:07:23<16:58, 15.66s/it]\n",
      " 66%|██████▌   | 125/189 [1:07:32<14:22, 13.47s/it]\n",
      " 67%|██████▋   | 126/189 [1:07:39<12:05, 11.52s/it]\n",
      "                                                   \n",
      "\n",
      " 67%|██████▋   | 126/189 [1:07:42<12:05, 11.52s/it]***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      " 15%|█▌        | 2/13 [00:07<00:39,  3.63s/it]\u001b[A\n",
      "\n",
      " 23%|██▎       | 3/13 [00:13<00:45,  4.60s/it]\u001b[A\n",
      "\n",
      " 31%|███       | 4/13 [00:19<00:46,  5.14s/it]\u001b[A\n",
      "\n",
      " 38%|███▊      | 5/13 [00:25<00:44,  5.54s/it]\u001b[A\n",
      "\n",
      " 46%|████▌     | 6/13 [00:32<00:42,  6.09s/it]\u001b[A\n",
      "\n",
      " 54%|█████▍    | 7/13 [00:40<00:39,  6.51s/it]\u001b[A\n",
      "\n",
      " 62%|██████▏   | 8/13 [00:47<00:33,  6.73s/it]\u001b[A\n",
      "\n",
      " 69%|██████▉   | 9/13 [00:50<00:22,  5.73s/it]\u001b[A\n",
      "\n",
      " 77%|███████▋  | 10/13 [00:57<00:18,  6.14s/it]\u001b[A\n",
      "\n",
      " 85%|████████▍ | 11/13 [01:05<00:12,  6.45s/it]\u001b[A\n",
      "\n",
      " 92%|█████████▏| 12/13 [01:12<00:06,  6.79s/it]\u001b[A\n",
      "\n",
      "100%|██████████| 13/13 [01:17<00:00,  6.14s/it]\u001b[A\n",
      "                                                   \n",
      "\n",
      "\n",
      "                                               \n",
      "\u001b[A\n",
      " 67%|██████▋   | 126/189 [1:09:42<12:05, 11.52s/it]\n",
      "\n",
      "100%|██████████| 13/13 [01:21<00:00,  6.14s/it]\u001b[A\n",
      "\n",
      "                                               \u001b[ASaving model checkpoint to D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-126\n",
      "Configuration saved in D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-126\\config.json\n",
      "Configuration saved in D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-126\\generation_config.json\n",
      "Model weights saved in D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-126\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-126\\tokenizer_config.json\n",
      "Special tokens file saved in D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-126\\special_tokens_map.json\n",
      "Copy vocab file to D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-126\\spiece.model\n",
      "\n",
      " 67%|██████▋   | 127/189 [1:14:15<2:11:15, 127.03s/it]\n",
      " 68%|██████▊   | 128/189 [1:14:24<1:33:01, 91.49s/it] \n",
      " 68%|██████▊   | 129/189 [1:14:32<1:06:36, 66.61s/it]\n",
      " 69%|██████▉   | 130/189 [1:14:43<48:55, 49.76s/it]  \n",
      " 69%|██████▉   | 131/189 [1:14:52<36:28, 37.73s/it]\n",
      " 70%|██████▉   | 132/189 [1:15:14<31:07, 32.76s/it]\n",
      " 70%|███████   | 133/189 [1:15:22<23:47, 25.48s/it]\n",
      " 71%|███████   | 134/189 [1:16:00<26:39, 29.08s/it]\n",
      " 71%|███████▏  | 135/189 [1:16:09<20:51, 23.18s/it]\n",
      " 72%|███████▏  | 136/189 [1:16:14<15:48, 17.90s/it]\n",
      " 72%|███████▏  | 137/189 [1:16:22<12:55, 14.90s/it]\n",
      " 73%|███████▎  | 138/189 [1:16:32<11:18, 13.30s/it]\n",
      " 74%|███████▎  | 139/189 [1:16:58<14:19, 17.18s/it]\n",
      " 74%|███████▍  | 140/189 [1:17:09<12:23, 15.18s/it]\n",
      " 75%|███████▍  | 141/189 [1:17:14<09:51, 12.31s/it]\n",
      " 75%|███████▌  | 142/189 [1:17:24<08:54, 11.38s/it]\n",
      " 76%|███████▌  | 143/189 [1:17:31<07:54, 10.32s/it]\n",
      " 76%|███████▌  | 144/189 [1:17:38<06:59,  9.31s/it]\n",
      " 77%|███████▋  | 145/189 [1:18:00<09:31, 12.99s/it]\n",
      " 77%|███████▋  | 146/189 [1:18:09<08:34, 11.96s/it]\n",
      " 78%|███████▊  | 147/189 [1:18:16<07:18, 10.45s/it]\n",
      " 78%|███████▊  | 148/189 [1:19:08<15:32, 22.76s/it]\n",
      " 79%|███████▉  | 149/189 [1:19:17<12:31, 18.80s/it]\n",
      " 79%|███████▉  | 150/189 [1:19:24<09:53, 15.21s/it]\n",
      " 80%|███████▉  | 151/189 [1:19:34<08:32, 13.49s/it]\n",
      " 80%|████████  | 152/189 [1:19:42<07:25, 12.03s/it]\n",
      " 81%|████████  | 153/189 [1:19:50<06:29, 10.81s/it]\n",
      " 81%|████████▏ | 154/189 [1:20:00<06:08, 10.52s/it]\n",
      " 82%|████████▏ | 155/189 [1:20:07<05:21,  9.45s/it]\n",
      " 83%|████████▎ | 156/189 [1:20:14<04:50,  8.80s/it]\n",
      " 83%|████████▎ | 157/189 [1:20:23<04:42,  8.82s/it]\n",
      " 84%|████████▎ | 158/189 [1:20:29<04:05,  7.92s/it]\n",
      " 84%|████████▍ | 159/189 [1:20:35<03:41,  7.37s/it]\n",
      " 85%|████████▍ | 160/189 [1:20:47<04:10,  8.63s/it]\n",
      " 85%|████████▌ | 161/189 [1:20:57<04:12,  9.03s/it]\n",
      " 86%|████████▌ | 162/189 [1:21:06<04:06,  9.14s/it]\n",
      " 86%|████████▌ | 163/189 [1:21:15<03:56,  9.11s/it]\n",
      " 87%|████████▋ | 164/189 [1:21:22<03:30,  8.42s/it]\n",
      " 87%|████████▋ | 165/189 [1:21:31<03:28,  8.68s/it]\n",
      " 88%|████████▊ | 166/189 [1:21:40<03:17,  8.61s/it]\n",
      " 88%|████████▊ | 167/189 [1:21:54<03:46, 10.31s/it]\n",
      " 89%|████████▉ | 168/189 [1:22:27<06:00, 17.17s/it]\n",
      " 89%|████████▉ | 169/189 [1:22:38<05:07, 15.40s/it]\n",
      " 90%|████████▉ | 170/189 [1:22:46<04:08, 13.07s/it]\n",
      " 90%|█████████ | 171/189 [1:22:52<03:14, 10.82s/it]\n",
      " 91%|█████████ | 172/189 [1:22:58<02:39,  9.39s/it]\n",
      " 92%|█████████▏| 173/189 [1:23:06<02:25,  9.09s/it]\n",
      " 92%|█████████▏| 174/189 [1:23:16<02:18,  9.21s/it]\n",
      " 93%|█████████▎| 175/189 [1:23:22<01:55,  8.28s/it]\n",
      " 93%|█████████▎| 176/189 [1:23:31<01:53,  8.72s/it]\n",
      " 94%|█████████▎| 177/189 [1:23:41<01:46,  8.91s/it]\n",
      " 94%|█████████▍| 178/189 [1:23:50<01:37,  8.87s/it]\n",
      " 95%|█████████▍| 179/189 [1:23:59<01:29,  8.93s/it]\n",
      " 95%|█████████▌| 180/189 [1:24:07<01:19,  8.78s/it]\n",
      " 96%|█████████▌| 181/189 [1:24:33<01:51, 13.90s/it]\n",
      " 96%|█████████▋| 182/189 [1:24:43<01:28, 12.65s/it]\n",
      " 97%|█████████▋| 183/189 [1:24:49<01:05, 10.88s/it]\n",
      " 97%|█████████▋| 184/189 [1:24:56<00:48,  9.69s/it]\n",
      " 98%|█████████▊| 185/189 [1:25:09<00:42, 10.69s/it]\n",
      " 98%|█████████▊| 186/189 [1:25:38<00:48, 16.08s/it]\n",
      " 99%|█████████▉| 187/189 [1:25:47<00:27, 13.96s/it]\n",
      " 99%|█████████▉| 188/189 [1:25:54<00:11, 11.90s/it]\n",
      "100%|██████████| 189/189 [1:26:02<00:00, 10.84s/it]\n",
      "                                                   \n",
      "\n",
      "100%|██████████| 189/189 [1:26:06<00:00, 10.84s/it]***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      " 15%|█▌        | 2/13 [00:06<00:35,  3.25s/it]\u001b[A\n",
      "\n",
      " 23%|██▎       | 3/13 [00:13<00:46,  4.61s/it]\u001b[A\n",
      "\n",
      " 31%|███       | 4/13 [00:18<00:45,  5.04s/it]\u001b[A\n",
      "\n",
      " 38%|███▊      | 5/13 [00:26<00:47,  5.92s/it]\u001b[A\n",
      "\n",
      " 46%|████▌     | 6/13 [00:34<00:46,  6.61s/it]\u001b[A\n",
      "\n",
      " 54%|█████▍    | 7/13 [00:40<00:39,  6.55s/it]\u001b[A\n",
      "\n",
      " 62%|██████▏   | 8/13 [00:47<00:32,  6.48s/it]\u001b[A\n",
      "\n",
      " 69%|██████▉   | 9/13 [00:50<00:22,  5.63s/it]\u001b[A\n",
      "\n",
      " 77%|███████▋  | 10/13 [00:55<00:15,  5.27s/it]\u001b[A\n",
      "\n",
      " 85%|████████▍ | 11/13 [01:00<00:10,  5.11s/it]\u001b[A\n",
      "\n",
      " 92%|█████████▏| 12/13 [01:05<00:05,  5.14s/it]\u001b[A\n",
      "\n",
      "100%|██████████| 13/13 [01:10<00:00,  5.04s/it]\u001b[A\n",
      "                                                   \n",
      "\n",
      "\n",
      "                                               \n",
      "\u001b[A\n",
      "100%|██████████| 189/189 [1:28:03<00:00, 10.84s/it]\n",
      "\n",
      "100%|██████████| 13/13 [01:21<00:00,  5.04s/it]\u001b[A\n",
      "\n",
      "                                               \u001b[ASaving model checkpoint to D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-189\n",
      "Configuration saved in D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-189\\config.json\n",
      "Configuration saved in D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-189\\generation_config.json\n",
      "Model weights saved in D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-189\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-189\\tokenizer_config.json\n",
      "Special tokens file saved in D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-189\\special_tokens_map.json\n",
      "Copy vocab file to D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-189\\spiece.model\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-63 (score: 0.0).\n",
      "\n",
      "                                                   \n",
      "\n",
      "100%|██████████| 189/189 [1:34:40<00:00, 10.84s/it]\n",
      "100%|██████████| 189/189 [1:34:55<00:00, 30.13s/it]\n",
      "Saving model checkpoint to D:/New folder/Model/content/AraT5_FT_question_generation\n",
      "Configuration saved in D:/New folder/Model/content/AraT5_FT_question_generation\\config.json\n",
      "Configuration saved in D:/New folder/Model/content/AraT5_FT_question_generation\\generation_config.json\n",
      "Model weights saved in D:/New folder/Model/content/AraT5_FT_question_generation\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/New folder/Model/content/AraT5_FT_question_generation\\tokenizer_config.json\n",
      "Special tokens file saved in D:/New folder/Model/content/AraT5_FT_question_generation\\special_tokens_map.json\n",
      "Copy vocab file to D:/New folder/Model/content/AraT5_FT_question_generation\\spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n",
      "\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]\n",
      " 15%|█▌        | 2/13 [00:27<02:28, 13.51s/it]\n",
      " 23%|██▎       | 3/13 [00:53<03:08, 18.82s/it]\n",
      " 31%|███       | 4/13 [01:18<03:10, 21.21s/it]\n",
      " 38%|███▊      | 5/13 [01:55<03:32, 26.54s/it]\n",
      " 46%|████▌     | 6/13 [02:19<03:00, 25.72s/it]\n",
      " 54%|█████▍    | 7/13 [02:42<02:30, 25.13s/it]\n",
      " 62%|██████▏   | 8/13 [03:08<02:06, 25.21s/it]\n",
      " 69%|██████▉   | 9/13 [03:35<01:43, 25.76s/it]\n",
      " 77%|███████▋  | 10/13 [03:55<01:12, 24.10s/it]\n",
      " 85%|████████▍ | 11/13 [04:20<00:48, 24.32s/it]\n",
      " 92%|█████████▏| 12/13 [04:44<00:24, 24.31s/it]\n",
      "100%|██████████| 13/13 [05:04<00:00, 22.93s/it]\n",
      "100%|██████████| 13/13 [05:18<00:00, 24.50s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 37.13it/s]\n",
      "loading configuration file config.json from cache at /tmp/AraT5_cache_dir\\models--UBC-NLP--AraT5-base\\snapshots\\ed49be981b4df4040e83de16fd559e191b87429f\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"UBC-NLP/AraT5-base\",\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"transformers_version\": \"4.27.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 110080\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /tmp/AraT5_cache_dir\\models--UBC-NLP--AraT5-base\\snapshots\\ed49be981b4df4040e83de16fd559e191b87429f\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"UBC-NLP/AraT5-base\",\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"transformers_version\": \"4.27.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 110080\n",
      "}\n",
      "\n",
      "loading file spiece.model from cache at /tmp/AraT5_cache_dir\\models--UBC-NLP--AraT5-base\\snapshots\\ed49be981b4df4040e83de16fd559e191b87429f\\spiece.model\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /tmp/AraT5_cache_dir\\models--UBC-NLP--AraT5-base\\snapshots\\ed49be981b4df4040e83de16fd559e191b87429f\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /tmp/AraT5_cache_dir\\models--UBC-NLP--AraT5-base\\snapshots\\ed49be981b4df4040e83de16fd559e191b87429f\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at /tmp/AraT5_cache_dir\\models--UBC-NLP--AraT5-base\\snapshots\\ed49be981b4df4040e83de16fd559e191b87429f\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"UBC-NLP/AraT5-base\",\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"transformers_version\": \"4.27.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 110080\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /tmp/AraT5_cache_dir\\models--UBC-NLP--AraT5-base\\snapshots\\ed49be981b4df4040e83de16fd559e191b87429f\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"UBC-NLP/AraT5-base\",\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"transformers_version\": \"4.27.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 110080\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /tmp/AraT5_cache_dir\\models--UBC-NLP--AraT5-base\\snapshots\\ed49be981b4df4040e83de16fd559e191b87429f\\pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.27.4\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at UBC-NLP/AraT5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "Generation config file not found, using a generation config created from the model config.\n",
      "\n",
      "Map:   0%|          | 0/100 [00:00<?, ? examples/s]c:\\Users\\obada\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3586: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "\n",
      "                                                   \n",
      "c:\\Users\\obada\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 500\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 189\n",
      "  Number of trainable parameters = 282770688\n",
      "\n",
      "  0%|          | 0/189 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      "  1%|          | 1/189 [00:52<2:43:19, 52.13s/it]\n",
      "  1%|          | 2/189 [03:41<6:17:08, 121.01s/it]\n",
      "  2%|▏         | 3/189 [07:19<8:32:25, 165.30s/it]\n",
      "  2%|▏         | 4/189 [08:18<6:20:34, 123.43s/it]\n",
      "  3%|▎         | 5/189 [08:37<4:22:51, 85.71s/it] \n",
      "  3%|▎         | 6/189 [08:50<3:06:31, 61.16s/it]\n",
      "  4%|▎         | 7/189 [09:01<2:15:29, 44.67s/it]\n",
      "  4%|▍         | 8/189 [09:10<1:40:27, 33.30s/it]\n",
      "  5%|▍         | 9/189 [09:18<1:16:18, 25.44s/it]\n",
      "  5%|▌         | 10/189 [09:25<58:53, 19.74s/it] \n",
      "  6%|▌         | 11/189 [09:32<47:08, 15.89s/it]\n",
      "  6%|▋         | 12/189 [09:40<39:52, 13.52s/it]\n",
      "  7%|▋         | 13/189 [09:48<34:27, 11.75s/it]\n",
      "  7%|▋         | 14/189 [09:57<32:05, 11.00s/it]\n",
      "  8%|▊         | 15/189 [10:07<30:43, 10.60s/it]\n",
      "  8%|▊         | 16/189 [10:15<27:52,  9.67s/it]\n",
      "  9%|▉         | 17/189 [10:23<26:18,  9.18s/it]\n",
      " 10%|▉         | 18/189 [10:30<24:32,  8.61s/it]\n",
      " 10%|█         | 19/189 [10:38<24:03,  8.49s/it]\n",
      " 11%|█         | 20/189 [10:46<23:28,  8.33s/it]\n",
      " 11%|█         | 21/189 [10:53<22:05,  7.89s/it]\n",
      " 12%|█▏        | 22/189 [11:01<22:18,  8.01s/it]\n",
      " 12%|█▏        | 23/189 [11:10<22:59,  8.31s/it]\n",
      " 13%|█▎        | 24/189 [11:20<23:54,  8.69s/it]\n",
      " 13%|█▎        | 25/189 [11:30<24:34,  8.99s/it]\n",
      " 14%|█▍        | 26/189 [11:40<25:37,  9.43s/it]\n",
      " 14%|█▍        | 27/189 [11:52<27:41, 10.26s/it]\n",
      " 15%|█▍        | 28/189 [12:03<28:09, 10.49s/it]\n",
      " 15%|█▌        | 29/189 [12:12<26:44, 10.03s/it]\n",
      " 16%|█▌        | 30/189 [12:28<30:47, 11.62s/it]\n",
      " 16%|█▋        | 31/189 [12:38<29:53, 11.35s/it]\n",
      " 17%|█▋        | 32/189 [12:50<30:08, 11.52s/it]\n",
      " 17%|█▋        | 33/189 [12:59<27:39, 10.64s/it]\n",
      " 18%|█▊        | 34/189 [13:06<24:42,  9.56s/it]\n",
      " 19%|█▊        | 35/189 [13:14<23:07,  9.01s/it]\n",
      " 19%|█▉        | 36/189 [13:29<28:16, 11.09s/it]\n",
      " 20%|█▉        | 37/189 [13:42<29:12, 11.53s/it]\n",
      " 20%|██        | 38/189 [13:52<28:01, 11.14s/it]\n",
      " 21%|██        | 39/189 [14:00<25:23, 10.16s/it]\n",
      " 21%|██        | 40/189 [14:10<25:07, 10.11s/it]\n",
      " 22%|██▏       | 41/189 [14:17<22:34,  9.15s/it]\n",
      " 22%|██▏       | 42/189 [14:27<23:12,  9.47s/it]\n",
      " 23%|██▎       | 43/189 [14:38<23:37,  9.71s/it]\n",
      " 23%|██▎       | 44/189 [14:48<24:07,  9.98s/it]\n",
      " 24%|██▍       | 45/189 [14:57<23:00,  9.58s/it]\n",
      " 24%|██▍       | 46/189 [15:03<20:31,  8.61s/it]\n",
      " 25%|██▍       | 47/189 [15:10<18:51,  7.97s/it]\n",
      " 25%|██▌       | 48/189 [15:16<17:40,  7.52s/it]\n",
      " 26%|██▌       | 49/189 [15:22<16:41,  7.15s/it]\n",
      " 26%|██▋       | 50/189 [15:29<16:20,  7.05s/it]\n",
      " 27%|██▋       | 51/189 [15:36<16:02,  6.97s/it]\n",
      " 28%|██▊       | 52/189 [15:44<16:18,  7.14s/it]\n",
      " 28%|██▊       | 53/189 [17:42<1:31:54, 40.55s/it]\n",
      " 29%|██▊       | 54/189 [18:33<1:38:36, 43.83s/it]\n",
      " 29%|██▉       | 55/189 [18:54<1:22:17, 36.85s/it]\n",
      " 30%|██▉       | 56/189 [19:09<1:07:19, 30.37s/it]\n",
      " 30%|███       | 57/189 [19:17<51:49, 23.55s/it]  \n",
      " 31%|███       | 58/189 [19:25<41:23, 18.96s/it]\n",
      " 31%|███       | 59/189 [19:34<34:10, 15.77s/it]\n",
      " 32%|███▏      | 60/189 [19:41<28:28, 13.25s/it]\n",
      " 32%|███▏      | 61/189 [19:50<25:36, 12.01s/it]\n",
      " 33%|███▎      | 62/189 [19:59<23:23, 11.05s/it]\n",
      " 33%|███▎      | 63/189 [20:05<20:04,  9.56s/it]\n",
      "                                                \n",
      "\n",
      " 33%|███▎      | 63/189 [20:10<20:04,  9.56s/it]***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.27.4\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      " 15%|█▌        | 2/13 [00:04<00:25,  2.36s/it]\u001b[A\n",
      "\n",
      " 23%|██▎       | 3/13 [00:09<00:34,  3.46s/it]\u001b[A\n",
      "\n",
      " 31%|███       | 4/13 [00:14<00:35,  3.98s/it]\u001b[A\n",
      "\n",
      " 38%|███▊      | 5/13 [00:20<00:37,  4.69s/it]\u001b[A\n",
      "\n",
      " 46%|████▌     | 6/13 [00:26<00:34,  4.95s/it]\u001b[A\n",
      "\n",
      " 54%|█████▍    | 7/13 [00:31<00:30,  5.10s/it]\u001b[A\n",
      "\n",
      " 62%|██████▏   | 8/13 [00:36<00:25,  5.19s/it]\u001b[A\n",
      "\n",
      " 69%|██████▉   | 9/13 [00:41<00:20,  5.08s/it]\u001b[A\n",
      "\n",
      " 77%|███████▋  | 10/13 [00:46<00:15,  5.02s/it]\u001b[A\n",
      "\n",
      " 85%|████████▍ | 11/13 [00:52<00:10,  5.17s/it]\u001b[A\n",
      "\n",
      " 92%|█████████▏| 12/13 [00:58<00:05,  5.40s/it]\u001b[A\n",
      "\n",
      "100%|██████████| 13/13 [01:01<00:00,  4.68s/it]\u001b[A\n",
      "                                                \n",
      "\n",
      "\n",
      "                                               \n",
      "\u001b[A\n",
      " 33%|███▎      | 63/189 [22:43<20:04,  9.56s/it]\n",
      "\n",
      "100%|██████████| 13/13 [02:00<00:00,  4.68s/it]\u001b[A\n",
      "\n",
      "                                               \u001b[ASaving model checkpoint to D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-63\n",
      "Configuration saved in D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-63\\config.json\n",
      "Configuration saved in D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-63\\generation_config.json\n",
      "Model weights saved in D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-63\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-63\\tokenizer_config.json\n",
      "Special tokens file saved in D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-63\\special_tokens_map.json\n",
      "Copy vocab file to D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-63\\spiece.model\n",
      "\n",
      " 34%|███▍      | 64/189 [59:29<24:50:31, 715.45s/it]\n",
      " 34%|███▍      | 65/189 [1:36:34<40:15:03, 1168.58s/it]\n",
      " 35%|███▍      | 66/189 [1:50:52<36:44:56, 1075.58s/it]\n",
      " 35%|███▌      | 67/189 [1:51:37<25:57:56, 766.20s/it] \n",
      " 36%|███▌      | 68/189 [1:51:47<18:07:46, 539.39s/it]\n",
      " 37%|███▋      | 69/189 [1:51:57<12:41:32, 380.77s/it]\n",
      " 37%|███▋      | 70/189 [1:52:08<8:55:11, 269.85s/it] \n",
      " 38%|███▊      | 71/189 [1:52:24<6:20:31, 193.49s/it]\n",
      " 38%|███▊      | 72/189 [1:52:37<4:32:09, 139.57s/it]\n",
      " 39%|███▊      | 73/189 [1:52:46<3:13:38, 100.16s/it]\n",
      " 39%|███▉      | 74/189 [1:52:56<2:20:20, 73.22s/it] \n",
      " 40%|███▉      | 75/189 [1:53:08<1:44:00, 54.74s/it]\n",
      " 40%|████      | 76/189 [1:53:19<1:18:49, 41.85s/it]\n",
      " 41%|████      | 77/189 [1:53:30<1:00:37, 32.48s/it]\n",
      " 41%|████▏     | 78/189 [1:53:41<48:01, 25.96s/it]  \n",
      " 42%|████▏     | 79/189 [1:53:52<39:35, 21.59s/it]\n",
      " 42%|████▏     | 80/189 [1:54:04<33:58, 18.70s/it]\n",
      " 43%|████▎     | 81/189 [1:54:14<28:57, 16.08s/it]\n",
      " 43%|████▎     | 82/189 [1:54:26<26:10, 14.68s/it]\n",
      " 44%|████▍     | 83/189 [1:54:36<23:26, 13.27s/it]\n",
      " 44%|████▍     | 84/189 [1:54:47<22:13, 12.70s/it]\n",
      " 45%|████▍     | 85/189 [1:54:58<21:06, 12.18s/it]\n",
      " 46%|████▌     | 86/189 [1:55:08<19:39, 11.45s/it]\n",
      " 46%|████▌     | 87/189 [1:55:19<19:15, 11.33s/it]\n",
      " 47%|████▋     | 88/189 [1:55:28<18:13, 10.82s/it]\n",
      " 47%|████▋     | 89/189 [1:55:38<17:41, 10.62s/it]\n",
      " 48%|████▊     | 90/189 [1:55:49<17:19, 10.50s/it]\n",
      " 48%|████▊     | 91/189 [1:55:58<16:48, 10.29s/it]\n",
      " 49%|████▊     | 92/189 [1:56:08<16:21, 10.12s/it]\n",
      " 49%|████▉     | 93/189 [1:56:18<16:01, 10.01s/it]\n",
      " 50%|████▉     | 94/189 [1:56:28<15:41,  9.91s/it]\n",
      " 50%|█████     | 95/189 [1:56:38<15:45, 10.06s/it]\n",
      " 51%|█████     | 96/189 [1:56:49<15:48, 10.20s/it]\n",
      " 51%|█████▏    | 97/189 [1:56:58<15:29, 10.11s/it]\n",
      " 52%|█████▏    | 98/189 [1:57:09<15:28, 10.21s/it]\n",
      " 52%|█████▏    | 99/189 [1:57:20<15:38, 10.43s/it]\n",
      " 53%|█████▎    | 100/189 [1:57:31<15:36, 10.53s/it]\n",
      " 53%|█████▎    | 101/189 [1:57:45<17:10, 11.71s/it]\n",
      " 54%|█████▍    | 102/189 [1:57:56<16:38, 11.48s/it]\n",
      " 54%|█████▍    | 103/189 [1:58:07<16:25, 11.46s/it]\n",
      " 55%|█████▌    | 104/189 [1:58:18<15:44, 11.11s/it]\n",
      " 56%|█████▌    | 105/189 [1:58:28<15:21, 10.97s/it]\n",
      " 56%|█████▌    | 106/189 [1:58:39<14:53, 10.76s/it]\n",
      " 57%|█████▋    | 107/189 [1:58:49<14:43, 10.77s/it]\n",
      " 57%|█████▋    | 108/189 [1:58:59<14:11, 10.52s/it]\n",
      " 58%|█████▊    | 109/189 [1:59:10<13:57, 10.47s/it]\n",
      " 58%|█████▊    | 110/189 [1:59:21<14:13, 10.80s/it]\n",
      " 59%|█████▊    | 111/189 [1:59:31<13:46, 10.60s/it]\n",
      " 59%|█████▉    | 112/189 [1:59:42<13:35, 10.59s/it]\n",
      " 60%|█████▉    | 113/189 [1:59:52<13:02, 10.30s/it]\n",
      " 60%|██████    | 114/189 [2:00:02<13:02, 10.43s/it]\n",
      " 61%|██████    | 115/189 [2:00:13<12:50, 10.41s/it]\n",
      " 61%|██████▏   | 116/189 [2:00:23<12:31, 10.30s/it]\n",
      " 62%|██████▏   | 117/189 [2:00:33<12:21, 10.29s/it]\n",
      " 62%|██████▏   | 118/189 [2:00:44<12:25, 10.50s/it]\n",
      " 63%|██████▎   | 119/189 [2:00:54<12:14, 10.50s/it]\n",
      " 63%|██████▎   | 120/189 [2:01:05<12:00, 10.45s/it]\n",
      " 64%|██████▍   | 121/189 [2:01:15<11:38, 10.27s/it]\n",
      " 65%|██████▍   | 122/189 [2:01:26<11:47, 10.56s/it]\n",
      " 65%|██████▌   | 123/189 [2:01:36<11:36, 10.55s/it]\n",
      " 66%|██████▌   | 124/189 [2:01:47<11:28, 10.60s/it]\n",
      " 66%|██████▌   | 125/189 [2:01:58<11:17, 10.59s/it]\n",
      " 67%|██████▋   | 126/189 [2:02:05<09:56,  9.47s/it]\n",
      "                                                   \n",
      "\n",
      " 67%|██████▋   | 126/189 [2:02:05<09:56,  9.47s/it]***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      " 15%|█▌        | 2/13 [00:07<00:43,  3.99s/it]\u001b[A\n",
      "\n",
      " 23%|██▎       | 3/13 [00:15<00:55,  5.55s/it]\u001b[A\n",
      "\n",
      " 31%|███       | 4/13 [00:22<00:53,  6.00s/it]\u001b[A\n",
      "\n",
      " 38%|███▊      | 5/13 [00:30<00:53,  6.69s/it]\u001b[A\n",
      "\n",
      " 46%|████▌     | 6/13 [00:38<00:49,  7.10s/it]\u001b[A\n",
      "\n",
      " 54%|█████▍    | 7/13 [00:45<00:42,  7.07s/it]\u001b[A\n",
      "\n",
      " 62%|██████▏   | 8/13 [00:52<00:35,  7.10s/it]\u001b[A\n",
      "\n",
      " 69%|██████▉   | 9/13 [00:56<00:24,  6.06s/it]\u001b[A\n",
      "\n",
      " 77%|███████▋  | 10/13 [01:02<00:18,  6.18s/it]\u001b[A\n",
      "\n",
      " 85%|████████▍ | 11/13 [01:09<00:12,  6.45s/it]\u001b[A\n",
      "\n",
      " 92%|█████████▏| 12/13 [01:16<00:06,  6.64s/it]\u001b[A\n",
      "\n",
      "100%|██████████| 13/13 [01:20<00:00,  5.84s/it]\u001b[A\n",
      "                                                   \n",
      "\n",
      "\n",
      "                                               \n",
      "\u001b[A\n",
      " 67%|██████▋   | 126/189 [2:03:35<09:56,  9.47s/it]\n",
      "\n",
      "100%|██████████| 13/13 [01:21<00:00,  5.84s/it]\u001b[A\n",
      "\n",
      "                                               \u001b[ASaving model checkpoint to D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-126\n",
      "Configuration saved in D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-126\\config.json\n",
      "Configuration saved in D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-126\\generation_config.json\n",
      "Model weights saved in D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-126\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-126\\tokenizer_config.json\n",
      "Special tokens file saved in D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-126\\special_tokens_map.json\n",
      "Copy vocab file to D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-126\\spiece.model\n",
      "\n",
      " 67%|██████▋   | 127/189 [2:04:29<51:33, 49.90s/it]\n",
      " 68%|██████▊   | 128/189 [2:04:38<38:24, 37.78s/it]\n",
      " 68%|██████▊   | 129/189 [2:04:48<29:22, 29.37s/it]\n",
      " 69%|██████▉   | 130/189 [2:04:58<23:16, 23.67s/it]\n",
      " 69%|██████▉   | 131/189 [2:05:08<18:46, 19.43s/it]\n",
      " 70%|██████▉   | 132/189 [2:05:18<15:42, 16.54s/it]\n",
      " 70%|███████   | 133/189 [2:05:28<13:42, 14.68s/it]\n",
      " 71%|███████   | 134/189 [2:05:37<11:58, 13.06s/it]\n",
      " 71%|███████▏  | 135/189 [2:05:47<10:45, 11.95s/it]\n",
      " 72%|███████▏  | 136/189 [2:05:56<09:47, 11.08s/it]\n",
      " 72%|███████▏  | 137/189 [2:06:05<09:11, 10.61s/it]\n",
      " 73%|███████▎  | 138/189 [2:06:15<08:49, 10.38s/it]\n",
      " 74%|███████▎  | 139/189 [2:06:24<08:21, 10.03s/it]\n",
      " 74%|███████▍  | 140/189 [2:06:34<08:03,  9.86s/it]\n",
      " 75%|███████▍  | 141/189 [2:06:43<07:42,  9.63s/it]\n",
      " 75%|███████▌  | 142/189 [2:06:53<07:35,  9.70s/it]\n",
      " 76%|███████▌  | 143/189 [2:07:02<07:22,  9.62s/it]\n",
      " 76%|███████▌  | 144/189 [2:07:12<07:09,  9.54s/it]\n",
      " 77%|███████▋  | 145/189 [2:07:22<07:08,  9.74s/it]\n",
      " 77%|███████▋  | 146/189 [2:07:31<06:52,  9.59s/it]\n",
      " 78%|███████▊  | 147/189 [2:07:40<06:40,  9.53s/it]\n",
      " 78%|███████▊  | 148/189 [2:07:50<06:28,  9.49s/it]\n",
      " 79%|███████▉  | 149/189 [2:07:59<06:14,  9.37s/it]\n",
      " 79%|███████▉  | 150/189 [2:08:08<06:00,  9.24s/it]\n",
      " 80%|███████▉  | 151/189 [2:08:17<05:49,  9.21s/it]\n",
      " 80%|████████  | 152/189 [2:08:26<05:38,  9.14s/it]\n",
      " 81%|████████  | 153/189 [2:08:35<05:29,  9.15s/it]\n",
      " 81%|████████▏ | 154/189 [2:08:44<05:16,  9.04s/it]\n",
      " 82%|████████▏ | 155/189 [2:08:53<05:12,  9.20s/it]\n",
      " 83%|████████▎ | 156/189 [2:09:03<05:04,  9.23s/it]\n",
      " 83%|████████▎ | 157/189 [2:09:12<04:55,  9.24s/it]\n",
      " 84%|████████▎ | 158/189 [2:09:21<04:44,  9.16s/it]\n",
      " 84%|████████▍ | 159/189 [2:09:31<04:38,  9.29s/it]\n",
      " 85%|████████▍ | 160/189 [2:09:41<04:34,  9.48s/it]\n",
      " 85%|████████▌ | 161/189 [2:09:53<04:47, 10.28s/it]\n",
      " 86%|████████▌ | 162/189 [2:10:03<04:37, 10.28s/it]\n",
      " 86%|████████▌ | 163/189 [2:10:13<04:23, 10.12s/it]\n",
      " 87%|████████▋ | 164/189 [2:10:22<04:06,  9.85s/it]\n",
      " 87%|████████▋ | 165/189 [2:10:31<03:54,  9.77s/it]\n",
      " 88%|████████▊ | 166/189 [2:10:41<03:40,  9.60s/it]\n",
      " 88%|████████▊ | 167/189 [2:10:49<03:25,  9.36s/it]\n",
      " 89%|████████▉ | 168/189 [2:10:59<03:16,  9.33s/it]\n",
      " 89%|████████▉ | 169/189 [2:11:07<03:02,  9.14s/it]\n",
      " 90%|████████▉ | 170/189 [2:11:17<02:53,  9.12s/it]\n",
      " 90%|█████████ | 171/189 [2:11:25<02:42,  9.04s/it]\n",
      " 91%|█████████ | 172/189 [2:11:34<02:32,  8.98s/it]\n",
      " 92%|█████████▏| 173/189 [2:11:44<02:29,  9.34s/it]\n",
      " 92%|█████████▏| 174/189 [2:11:53<02:18,  9.25s/it]\n",
      " 93%|█████████▎| 175/189 [2:12:03<02:09,  9.25s/it]\n",
      " 93%|█████████▎| 176/189 [2:12:13<02:03,  9.51s/it]\n",
      " 94%|█████████▎| 177/189 [2:12:22<01:53,  9.47s/it]\n",
      " 94%|█████████▍| 178/189 [2:12:31<01:43,  9.41s/it]\n",
      " 95%|█████████▍| 179/189 [2:12:42<01:36,  9.61s/it]\n",
      " 95%|█████████▌| 180/189 [2:12:51<01:25,  9.48s/it]\n",
      " 96%|█████████▌| 181/189 [2:13:00<01:15,  9.45s/it]\n",
      " 96%|█████████▋| 182/189 [2:13:09<01:05,  9.43s/it]\n",
      " 97%|█████████▋| 183/189 [2:13:19<00:56,  9.41s/it]\n",
      " 97%|█████████▋| 184/189 [2:13:29<00:48,  9.66s/it]\n",
      " 98%|█████████▊| 185/189 [2:13:38<00:38,  9.50s/it]\n",
      " 98%|█████████▊| 186/189 [2:13:47<00:28,  9.39s/it]\n",
      " 99%|█████████▉| 187/189 [2:13:57<00:19,  9.50s/it]\n",
      " 99%|█████████▉| 188/189 [2:14:06<00:09,  9.45s/it]\n",
      "100%|██████████| 189/189 [2:14:12<00:00,  8.40s/it]\n",
      "                                                   \n",
      "\n",
      "100%|██████████| 189/189 [2:14:12<00:00,  8.40s/it]***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      " 15%|█▌        | 2/13 [00:07<00:38,  3.55s/it]\u001b[A\n",
      "\n",
      " 23%|██▎       | 3/13 [00:12<00:44,  4.41s/it]\u001b[A\n",
      "\n",
      " 31%|███       | 4/13 [00:18<00:45,  5.07s/it]\u001b[A\n",
      "\n",
      " 38%|███▊      | 5/13 [00:26<00:46,  5.85s/it]\u001b[A\n",
      "\n",
      " 46%|████▌     | 6/13 [00:33<00:44,  6.31s/it]\u001b[A\n",
      "\n",
      " 54%|█████▍    | 7/13 [00:40<00:39,  6.51s/it]\u001b[A\n",
      "\n",
      " 62%|██████▏   | 8/13 [00:46<00:32,  6.42s/it]\u001b[A\n",
      "\n",
      " 69%|██████▉   | 9/13 [00:50<00:22,  5.60s/it]\u001b[A\n",
      "\n",
      " 77%|███████▋  | 10/13 [00:54<00:15,  5.14s/it]\u001b[A\n",
      "\n",
      " 85%|████████▍ | 11/13 [00:59<00:10,  5.02s/it]\u001b[A\n",
      "\n",
      " 92%|█████████▏| 12/13 [01:03<00:04,  4.94s/it]\u001b[A\n",
      "\n",
      "100%|██████████| 13/13 [01:07<00:00,  4.65s/it]\u001b[A\n",
      "                                                   \n",
      "\n",
      "\n",
      "                                               \n",
      "\u001b[A\n",
      "100%|██████████| 189/189 [2:15:27<00:00,  8.40s/it]\n",
      "\n",
      "100%|██████████| 13/13 [01:07<00:00,  4.65s/it]\u001b[A\n",
      "\n",
      "                                               \u001b[ASaving model checkpoint to D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-189\n",
      "Configuration saved in D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-189\\config.json\n",
      "Configuration saved in D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-189\\generation_config.json\n",
      "Model weights saved in D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-189\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-189\\tokenizer_config.json\n",
      "Special tokens file saved in D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-189\\special_tokens_map.json\n",
      "Copy vocab file to D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-189\\spiece.model\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from D:/New folder/Model/content/AraT5_FT_question_generation\\checkpoint-63 (score: 0.0).\n",
      "\n",
      "                                                   \n",
      "\n",
      "100%|██████████| 189/189 [2:16:19<00:00,  8.40s/it]\n",
      "100%|██████████| 189/189 [2:16:21<00:00, 43.29s/it]\n",
      "Saving model checkpoint to D:/New folder/Model/content/AraT5_FT_question_generation\n",
      "Configuration saved in D:/New folder/Model/content/AraT5_FT_question_generation\\config.json\n",
      "Configuration saved in D:/New folder/Model/content/AraT5_FT_question_generation\\generation_config.json\n",
      "Model weights saved in D:/New folder/Model/content/AraT5_FT_question_generation\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/New folder/Model/content/AraT5_FT_question_generation\\tokenizer_config.json\n",
      "Special tokens file saved in D:/New folder/Model/content/AraT5_FT_question_generation\\special_tokens_map.json\n",
      "Copy vocab file to D:/New folder/Model/content/AraT5_FT_question_generation\\spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n",
      "\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]\n",
      " 15%|█▌        | 2/13 [00:25<02:18, 12.55s/it]\n",
      " 23%|██▎       | 3/13 [00:50<02:58, 17.87s/it]\n",
      " 31%|███       | 4/13 [01:13<02:58, 19.81s/it]\n",
      " 38%|███▊      | 5/13 [01:38<02:52, 21.54s/it]\n",
      " 46%|████▌     | 6/13 [02:03<02:38, 22.61s/it]\n",
      " 54%|█████▍    | 7/13 [02:27<02:19, 23.28s/it]\n",
      " 62%|██████▏   | 8/13 [02:52<01:58, 23.71s/it]\n",
      " 69%|██████▉   | 9/13 [03:15<01:34, 23.57s/it]\n",
      " 77%|███████▋  | 10/13 [03:39<01:11, 23.71s/it]\n",
      " 85%|████████▍ | 11/13 [04:04<00:48, 24.04s/it]\n",
      " 92%|█████████▏| 12/13 [04:29<00:24, 24.30s/it]\n",
      "100%|██████████| 13/13 [04:46<00:00, 22.23s/it]\n",
      "100%|██████████| 13/13 [04:46<00:00, 22.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_checkpoint None\n",
      "04/06/2023 04:35:18 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
      "04/06/2023 04:35:18 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=True,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=D:/New folder/Model/content/AraT5_FT_question_generation\\runs\\Apr06_04-35-18_LAPTOP-NQ25E69Q,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=epoch,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=eval_bleu,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=D:/New folder/Model/content/AraT5_FT_question_generation,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=D:/New folder/Model/content/AraT5_FT_question_generation,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "04/06/2023 04:35:20 - WARNING - datasets.builder -   Found cached dataset json (d:/tmp/AraT5_cache_dir/json/default-b59a04ea7a6236d9/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n",
      "04/06/2023 04:35:30 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at d:\\tmp\\AraT5_cache_dir\\json\\default-b59a04ea7a6236d9\\0.0.0\\fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\\cache-d6027ecf3f030aeb.arrow\n",
      "[INFO] evlaute using  bleu score task name: question_generation\n",
      "[INFO] early_stopping_num= 20\n",
      "***** checkpoint= None\n",
      "{'loss': 34.8319, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n",
      "{'eval_loss': 26.341596603393555, 'eval_bleu': 0.0, 'eval_gen_len': 19.0, 'eval_runtime': 137.8018, 'eval_samples_per_second': 0.726, 'eval_steps_per_second': 0.094, 'epoch': 1.0}\n",
      "{'loss': 18.1129, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n",
      "{'eval_loss': 14.074260711669922, 'eval_bleu': 0.0, 'eval_gen_len': 15.69, 'eval_runtime': 89.9465, 'eval_samples_per_second': 1.112, 'eval_steps_per_second': 0.145, 'epoch': 2.0}\n",
      "{'loss': 13.358, 'learning_rate': 0.0, 'epoch': 3.0}\n",
      "{'eval_loss': 12.144558906555176, 'eval_bleu': 0.0, 'eval_gen_len': 5.06, 'eval_runtime': 75.1291, 'eval_samples_per_second': 1.331, 'eval_steps_per_second': 0.173, 'epoch': 3.0}\n",
      "{'train_runtime': 8179.4966, 'train_samples_per_second': 0.183, 'train_steps_per_second': 0.023, 'train_loss': 22.100942137380127, 'epoch': 3.0}\n",
      "04/06/2023 06:52:02 - INFO - __main__ -   ***** train metrics *****\n",
      "04/06/2023 06:52:02 - INFO - __main__ -     epoch                    =        3.0\n",
      "04/06/2023 06:52:02 - INFO - __main__ -     train_loss               =    22.1009\n",
      "04/06/2023 06:52:02 - INFO - __main__ -     train_runtime            = 2:16:19.49\n",
      "04/06/2023 06:52:02 - INFO - __main__ -     train_samples            =        500\n",
      "04/06/2023 06:52:02 - INFO - __main__ -     train_samples_per_second =      0.183\n",
      "04/06/2023 06:52:02 - INFO - __main__ -     train_steps_per_second   =      0.023\n",
      "04/06/2023 06:52:03 - INFO - __main__ -   *** Evaluate ***\n",
      "04/06/2023 06:57:15 - INFO - __main__ -   ***** val metrics *****\n",
      "04/06/2023 06:57:15 - INFO - __main__ -     epoch                   =        3.0\n",
      "04/06/2023 06:57:15 - INFO - __main__ -     eval_bleu               =        0.0\n",
      "04/06/2023 06:57:15 - INFO - __main__ -     eval_check_point        = AraT5-base\n",
      "04/06/2023 06:57:15 - INFO - __main__ -     eval_gen_len            =      127.0\n",
      "04/06/2023 06:57:15 - INFO - __main__ -     eval_loss               =    26.3416\n",
      "04/06/2023 06:57:15 - INFO - __main__ -     eval_runtime            = 0:05:12.40\n",
      "04/06/2023 06:57:15 - INFO - __main__ -     eval_samples            =        100\n",
      "04/06/2023 06:57:15 - INFO - __main__ -     eval_samples_per_second =       0.32\n",
      "04/06/2023 06:57:15 - INFO - __main__ -     eval_steps_per_second   =      0.042\n"
     ]
    }
   ],
   "source": [
    "!python run_trainier_seq2seq_huggingface.py \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --max_target_length 128 --max_source_length 128 \\\n",
    "    --per_device_train_batch_size 8 --per_device_eval_batch_size 8 \\\n",
    "    --model_name_or_path \"UBC-NLP/AraT5-base\" \\\n",
    "    --output_dir \"D:/New folder/Model/content/AraT5_FT_question_generation\" --overwrite_output_dir \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --train_file \"D:/New folder/Model/train_qg_small.json\" \\\n",
    "    --validation_file \"D:/New folder/Model/valid_qg_small.json\" \\\n",
    "    --task \"question_generation\" \\\n",
    "    --load_best_model_at_end --metric_for_best_model \"eval_bleu\" --greater_is_better True --evaluation_strategy epoch --logging_strategy epoch --save_strategy epoch --predict_with_generate \\\n",
    "    --do_train --do_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU available:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python run_trainier_seq2seq_huggingface.py --learning_rate 5e-5 --max_target_length 128 --max_source_length 128 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --model_name_or_path \"UBC-NLP/AraT5-base\" --output_dir \"D:/New folder/Model/content/AraT5_FT_question_generation\" --overwrite_output_dir --num_train_epochs 4 --train_file \"D:/New folder/Model/train_qg_small.json\" --validation_file \"D:/New folder/Model/valid_qg_small.json\" --task \"question_generation\" --load_best_model_at_end --metric_for_best_model \"eval_bleu\" --greater_is_better True --evaluation_strategy steps --eval_steps 500 --logging_strategy steps --logging_steps 100 --save_strategy steps --save_steps 1000 --predict_with_generate --do_train --do_eval\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
